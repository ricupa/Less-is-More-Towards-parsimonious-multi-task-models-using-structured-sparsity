
##### screen -ls | grep Detached | cut -d. -f1 | awk '{print $1}' | xargs kill


##### conda activate MTLmetaenv
### python main_multi_trials.py --config_exp config/exp_single_multi_celebA.yaml

# ####class_glasses      #### very high imbalance so not using this task 
# 1. segmentsemantic
# 3. class_male
# 4. class_eyebrows
# 5. class_smile
# 6 multi_seg_male 
# 7 multi_seg_smile   #### not using as of now 
# 8 multi_male_eyebrows   #### not using as of now 
# 9 multi_male_eyebrows_smile
# 10 multi_seg_male_eyebrows_smile

'Experiment_name': '11_1_multi_male_smile_1e5__' 
'dataset_name' : 'celebA'  ###### 'NYU', 'celebA'
'task_list': ['class_male','class_smile']  ###   'class_male' , 'class_eyebrows', 'class_smile', 'segmentsemantic', 'class_glasses'
'setup': 'multitask' #### 'multitask', 'singletask'
'group_sparsity' : True
'sparsity_threshold': 0
'backbone' : 'resnetd50' #### resnetd50,resnetd101
'checkpoint': False 
'checkpoint_folder': '/home/ricupa/Documents/MTL_meta_adaptive_features/MTL_adaptive_results/new/'
'wandb_img_log': False
'num_trials': 2
# 'lambda_list' : [0, 0.000001, 0.00001, 0.0001]
### [0.001, 0.0001, 0.00001, 0.000001, 0.0000001]
##### [0.001, 0.0001, 0.00001, 0.000001]



# COMBINE LOSSES
'comb_loss' : 'uncertainity'   # 'uncertainity', 'sum', 'wt_sum'

# BATCH SPECS
'train_batch_size' : 32   
'val_batch_size' : 32
'test_batch_size' : 32

# HYPERPARAMETERS

'input_shape': 256
'epochs' : 1000
'num_workers': 8
'earlystop_patience': 10              ### keep greater than 10 since the scheduler patience is 10
'task_earlystop_patience' : 10        ### keep greater than 10 since the scheduler patience is 10
'input_img_channels': 3


# LOSS FUNCTION
### seg loss function (for example)
'seg_loss_fn': 'softCEloss'   ####  dice, Tversky,  softCEloss, Focal


#  INITIAL NON SPARSE OPTIMIZER PARAMETERS
'optimizer': 'adam'    ### 'sgd', 'adamw' , 'adam' 
'optimizer_params': 
    'learning_rate': 0.0001
    'betas': [0.9, 0.999]
    'weight_decay' : 0.01
    'penalty' : 'l1_l2'
    'lambda' : 0.0001   #### not being used 

# BACKBONE OPTIMIZER PARAMETERS
'bb_optimizer': 'adam'    ### 'sgd', 'adamw' , 'adam'  #### in case of sparsity it is always ADAMW
'bb_optimizer_params': 
    'learning_rate': 0.0001 #####0.00001
    'betas': [0.9, 0.999]
    'weight_decay' : 0.1
    'penalty' : 'l1_l2'
    'lambda' : 0.00001 #### for sparsity 
    
#### DATA 
'num_input_ch' : 3
'data_dir_NYU' : "/home/ricupa/Documents/M3TL/NYU_dataset/NYUD_MT"
'data_dir_celebA' : "/home/ricupa/Documents/MTL_meta_adaptive_features/data/CelebAMask-HQ"
